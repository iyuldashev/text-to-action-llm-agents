# Text to Action: Large Language Models into Multimodal Agents

This repository contains a small-scale research study and experiment by **Yuldashev Izzatillo**, exploring how Large Language Models (LLMs) act as high-level planners in multimodal agents through systems like SayCan, PaLM-E, and Gato.

## üß† Overview

Multimodal agents powered by LLMs can interpret language, vision, and sensory input to perform tasks in the real world. This project highlights:
- A review of key LLM-agent architectures (SayCan, PaLM-E, Gato)
- A GPT-4o experiment mapping natural language tasks to action plans
- Reflections on how LLMs enable semantic grounding and action decomposition

## üß™ Sample Task Planning (JSON)

Includes real prompts like:
- "I spilled my drink"
- "Can you organize the books?"
- "Put the dishes in the sink"

...and their corresponding action plans generated by GPT-4o.

## üìú License

MIT License (see `LICENSE` file)

## ‚úçÔ∏è Citation

```bibtex
@misc{yuldashev2025text2action,
  title={Text to Action: Large Language Models into Multimodal Agents},
  author={Yuldashev, Izzatillo},
  institution={Pukyong National University},
  year={2025},
  note={Available at: https://github.com/iyuldashev/text-to-action-llm-agents}
}
